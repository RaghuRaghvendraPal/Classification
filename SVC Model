# Data Preprocessing Template
#########################################SVM Classification##################################################################
#########################SVM Classification non linear in curve###############
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# Importing the dataset
data = {
"User ID":	[15624510,	15810944,	15668575,	15603246,	15804002,	15728773,	15598044,	15694829,	15600575,	15727311,	15570769,	15606274,	15746139,	15704987,	15628972,	15697686,	15733883,	15617482,	15704583,	15621083,	15649487,	15736760,	15714658,	15599081,	15705113,	15631159,	15792818,	15633531,	15744529,	15669656,	15581198,	15729054,	15573452,	15776733,	15724858,	15713144,	15690188,	15689425,	15671766,	15782806,	15764419,	15591915,	15772798,	15792008,	15715541,	15639277,	15798850,	15776348,	15727696,	15793813,	15694395,	15764195,	15744919,	15671655,	15654901,	15649136,	15775562,	15807481,	15642885,	15789109,	15814004,	15673619,	15595135,	15583681,	15605000,	15718071,	15679760,	15654574,	15577178,	15595324,	15756932,	15726358,	15595228,	15782530,	15592877,	15651983,	15746737,	15774179,	15667265,	15655123,	15595917,	15668385,	15709476,	15711218,	15798659,	15663939,	15694946,	15631912,	15768816,	15682268,	15684801,	15636428,	15809823,	15699284,	15786993,	15709441,	15710257,	15582492,	15575694,	15756820,	15766289,	15593014,	15584545,	15675949,	15672091,	15801658,	15706185,	15789863,	15720943,	15697997,	15665416,	15660200,	15619653,	15773447,	15739160,	15689237,	15679297,	15591433,	15642725,	15701962,	15811613,	15741049,	15724423,	15574305,	15678168,	15697020,	15610801,	15745232,	15722758,	15792102,	15675185,	15801247,	15725660,	15638963,	15800061,	15578006,	15668504,	15687491,	15610403,	15741094,	15807909,	15666141,	15617134,	15783029,	15622833,	15746422,	15750839,	15749130,	15779862,	15767871,	15679651,	15576219,	15699247,	15619087,	15605327,	15610140,	15791174,	15602373,	15762605,	15598840,	15744279,	15670619,	15599533,	15757837,	15697574,	15578738,	15762228,	15614827,	15789815,	15579781,	15587013,	15570932,	15794661,	15581654,	15644296,	15614420,	15609653,	15594577,	15584114,	15673367,	15685576,	15774727,	15694288,	15603319,	15759066,	15814816,	15724402,	15571059,	15674206,	15715160,	15730448,	15662067,	15779581,	15662901,	15689751,	15667742,	15738448,	15680243,	15745083,	15708228,	15628523,	15708196,	15735549,	15809347,	15660866,	15766609,	15654230,	15794566,	15800890,	15697424,	15724536,	15735878,	15707596,	15657163,	15622478,	15779529,	15636023,	15582066,	15666675,	15732987,	15789432,	15663161,	15694879,	15593715,	15575002,	15622171,	15795224,	15685346,	15691808,	15721007,	15794253,	15694453,	15813113,	15614187,	15619407,	15646227,	15660541,	15753874,	15617877,	15772073,	15701537,	15736228,	15780572,	15769596,	15586996,	15722061,	15638003,	15775590,	15730688,	15753102,	15810075,	15723373,	15795298,	15584320,	15724161,	15750056,	15609637,	15794493,	15569641,	15815236,	15811177,	15680587,	15672821,	15767681,	15600379,	15801336,	15721592,	15581282,	15746203,	15583137,	15680752,	15688172,	15791373,	15589449,	15692819,	15727467,	15734312,	15764604,	15613014,	15759684,	15609669,	15685536,	15750447,	15663249,	15638646,	15734161,	15631070,	15761950,	15649668,	15713912,	15586757,	15596522,	15625395,	15760570,	15566689,	15725794,	15673539,	15705298,	15675791,	15747043,	15736397,	15678201,	15720745,	15637593,	15598070,	15787550,	15603942,	15733973,	15596761,	15652400,	15717893,	15622585,	15733964,	15753861,	15747097,	15594762,	15667417,	15684861,	15742204,	15623502,	15774872,	15611191,	15674331,	15619465,	15575247,	15695679,	15713463,	15785170,	15796351,	15639576,	15693264,	15589715,	15769902,	15587177,	15814553,	15601550,	15664907,	15612465,	15810800,	15665760,	15588080,	15776844,	15717560,	15629739,	15729908,	15716781,	15646936,	15768151,	15579212,	15721835,	15800515,	15591279,	15587419,	15750335,	15699619,	15606472,	15778368,	15671387,	15573926,	15709183,	15577514,	15778830,	15768072,	15768293,	15654456,	15807525,	15574372,	15671249,	15779744,	15624755,	15611430,	15774744,	15629885,	15708791,	15793890,	15646091,	15596984,	15800215,	15577806,	15749381,	15683758,	15670615,	15715622,	15707634,	15806901,	15775335,	15724150,	15627220,	15672330,	15668521,	15807837,	15592570,	15748589,	15635893,	15757632,	15691863,	15706071,	15654296,	15755018,	15594041],
"Gender":["Male",	"Male",	"Female",	"Female",	"Male",	"Male",	"Female",	"Female",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Male",	"Male",	"Male",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Male",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Male",	"Male",	"Male",	"Male",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Female",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Male",	"Male",	"Male",	"Female",	"Female",	"Male",	"Male",	"Male",	"Male",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Female",	"Female",	"Female",	"Female",	"Female",	"Male",	"Female",	"Female",	"Male",	"Female",	"Female",	"Female",	"Female",	"Female",	"Male",	"Female",	"Female",	"Female",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Male",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Female",	"Female",	"Male",	"Female",	"Female",	"Female",	"Female",	"Female",	"Male",	"Male",	"Male",	"Female",	"Female",	"Male",	"Female",	"Female",	"Female",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Male",	"Male",	"Female",	"Male",	"Male",	"Male",	"Female",	"Female",	"Female",	"Male",	"Female",	"Female",	"Male",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Female",	"Female",	"Female",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Male",	"Female",	"Male",	"Female",	"Female",	"Male",	"Female",	"Male",	"Female"],
"Age":[19,	35,	26,	27,	19,	27,	27,	32,	25,	35,	26,	26,	20,	32,	18,	29,	47,	45,	46,	48,	45,	47,	48,	45,	46,	47,	49,	47,	29,	31,	31,	27,	21,	28,	27,	35,	33,	30,	26,	27,	27,	33,	35,	30,	28,	23,	25,	27,	30,	31,	24,	18,	29,	35,	27,	24,	23,	28,	22,	32,	27,	25,	23,	32,	59,	24,	24,	23,	22,	31,	25,	24,	20,	33,	32,	34,	18,	22,	28,	26,	30,	39,	20,	35,	30,	31,	24,	28,	26,	35,	22,	30,	26,	29,	29,	35,	35,	28,	35,	28,	27,	28,	32,	33,	19,	21,	26,	27,	26,	38,	39,	37,	38,	37,	42,	40,	35,	36,	40,	41,	36,	37,	40,	35,	41,	39,	42,	26,	30,	26,	31,	33,	30,	21,	28,	23,	20,	30,	28,	19,	19,	18,	35,	30,	34,	24,	27,	41,	29,	20,	26,	41,	31,	36,	40,	31,	46,	29,	26,	32,	32,	25,	37,	35,	33,	18,	22,	35,	29,	29,	21,	34,	26,	34,	34,	23,	35,	25,	24,	31,	26,	31,	32,	33,	33,	31,	20,	33,	35,	28,	24,	19,	29,	19,	28,	34,	30,	20,	26,	35,	35,	49,	39,	41,	58,	47,	55,	52,	40,	46,	48,	52,	59,	35,	47,	60,	49,	40,	46,	59,	41,	35,	37,	60,	35,	37,	36,	56,	40,	42,	35,	39,	40,	49,	38,	46,	40,	37,	46,	53,	42,	38,	50,	56,	41,	51,	35,	57,	41,	35,	44,	37,	48,	37,	50,	52,	41,	40,	58,	45,	35,	36,	55,	35,	48,	42,	40,	37,	47,	40,	43,	59,	60,	39,	57,	57,	38,	49,	52,	50,	59,	35,	37,	52,	48,	37,	37,	48,	41,	37,	39,	49,	55,	37,	35,	36,	42,	43,	45,	46,	58,	48,	37,	37,	40,	42,	51,	47,	36,	38,	42,	39,	38,	49,	39,	39,	54,	35,	45,	36,	52,	53,	41,	48,	48,	41,	41,	42,	36,	47,	38,	48,	42,	40,	57,	36,	58,	35,	38,	39,	53,	35,	38,	47,	47,	41,	53,	54,	39,	38,	38,	37,	42,	37,	36,	60,	54,	41,	40,	42,	43,	53,	47,	42,	42,	59,	58,	46,	38,	54,	60,	60,	39,	59,	37,	46,	46,	42,	41,	58,	42,	48,	44,	49,	57,	56,	49,	39,	47,	48,	48,	47,	45,	60,	39,	46,	51,	50,	36,	49]
,"EstimatedSalary":[19000,	20000,	43000,	57000,	76000,	58000,	84000,	150000,	33000,	65000,	80000,	52000,	86000,	18000,	82000,	80000,	25000,	26000,	28000,	29000,	22000,	49000,	41000,	22000,	23000,	20000,	28000,	30000,	43000,	18000,	74000,	137000,	16000,	44000,	90000,	27000,	28000,	49000,	72000,	31000,	17000,	51000,	108000,	15000,	84000,	20000,	79000,	54000,	135000,	89000,	32000,	44000,	83000,	23000,	58000,	55000,	48000,	79000,	18000,	117000,	20000,	87000,	66000,	120000,	83000,	58000,	19000,	82000,	63000,	68000,	80000,	27000,	23000,	113000,	18000,	112000,	52000,	27000,	87000,	17000,	80000,	42000,	49000,	88000,	62000,	118000,	55000,	85000,	81000,	50000,	81000,	116000,	15000,	28000,	83000,	44000,	25000,	123000,	73000,	37000,	88000,	59000,	86000,	149000,	21000,	72000,	35000,	89000,	86000,	80000,	71000,	71000,	61000,	55000,	80000,	57000,	75000,	52000,	59000,	59000,	75000,	72000,	75000,	53000,	51000,	61000,	65000,	32000,	17000,	84000,	58000,	31000,	87000,	68000,	55000,	63000,	82000,	107000,	59000,	25000,	85000,	68000,	59000,	89000,	25000,	89000,	96000,	30000,	61000,	74000,	15000,	45000,	76000,	50000,	47000,	15000,	59000,	75000,	30000,	135000,	100000,	90000,	33000,	38000,	69000,	86000,	55000,	71000,	148000,	47000,	88000,	115000,	118000,	43000,	72000,	28000,	47000,	22000,	23000,	34000,	16000,	71000,	117000,	43000,	60000,	66000,	82000,	41000,	72000,	32000,	84000,	26000,	43000,	70000,	89000,	43000,	79000,	36000,	80000,	22000,	39000,	74000,	134000,	71000,	101000,	47000,	130000,	114000,	142000,	22000,	96000,	150000,	42000,	58000,	43000,	108000,	65000,	78000,	96000,	143000,	80000,	91000,	144000,	102000,	60000,	53000,	126000,	133000,	72000,	80000,	147000,	42000,	107000,	86000,	112000,	79000,	57000,	80000,	82000,	143000,	149000,	59000,	88000,	104000,	72000,	146000,	50000,	122000,	52000,	97000,	39000,	52000,	134000,	146000,	44000,	90000,	72000,	57000,	95000,	131000,	77000,	144000,	125000,	72000,	90000,	108000,	75000,	74000,	144000,	61000,	133000,	76000,	42000,	106000,	26000,	74000,	71000,	88000,	38000,	36000,	88000,	61000,	70000,	21000,	141000,	93000,	62000,	138000,	79000,	78000,	134000,	89000,	39000,	77000,	57000,	63000,	73000,	112000,	79000,	117000,	38000,	74000,	137000,	79000,	60000,	54000,	134000,	113000,	125000,	50000,	70000,	96000,	50000,	141000,	79000,	75000,	104000,	55000,	32000,	60000,	138000,	82000,	52000,	30000,	131000,	60000,	72000,	75000,	118000,	107000,	51000,	119000,	65000,	65000,	60000,	54000,	144000,	79000,	55000,	122000,	104000,	75000,	65000,	51000,	105000,	63000,	72000,	108000,	77000,	61000,	113000,	75000,	90000,	57000,	99000,	34000,	70000,	72000,	71000,	54000,	129000,	34000,	50000,	79000,	104000,	29000,	47000,	88000,	71000,	26000,	46000,	83000,	73000,	130000,	80000,	32000,	74000,	53000,	87000,	23000,	64000,	33000,	139000,	28000,	33000,	60000,	39000,	71000,	34000,	35000,	33000,	23000,	45000,	42000,	59000,	41000,	23000,	20000,	33000,	36000],
"Purchased":[0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	1,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	1,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	0,	1,	0,	1,	0,	1,	0,	1,	0,	1,	1,	0,	0,	0,	1,	0,	0,	0,	1,	0,	1,	1,	1,	0,	0,	1,	1,	0,	1,	1,	0,	1,	1,	0,	1,	0,	0,	0,	1,	1,	0,	1,	1,	0,	1,	0,	1,	0,	1,	0,	0,	1,	1,	0,	1,	0,	0,	1,	1,	0,	1,	1,	0,	1,	1,	0,	0,	1,	0,	0,	1,	1,	1,	1,	1,	0,	1,	1,	1,	1,	0,	1,	1,	0,	1,	0,	1,	0,	1,	1,	1,	1,	0,	0,	0,	1,	1,	0,	1,	1,	1,	1,	1,	0,	0,	0,	1,	1,	0,	0,	1,	0,	1,	0,	1,	1,	0,	1,	0,	1,	1,	0,	1,	1,	0,	0,	0,	1,	1,	0,	1,	0,	0,	1,	0,	1,	0,	0,	1,	1,	0,	0,	1,	1,	0,	1,	1,	0,	0,	1,	0,	1,	0,	1,	1,	1,	0,	1,	0,	1,	1,	1,	0,	1,	1,	1,	1,	0,	1,	1,	1,	0,	1,	0,	1,	0,	0,	1,	1,	0,	1,	1,	1,	1,	1,	1,	0,	1,	1,	1,	1,	1,	1,	0,	1,	1,	1,	0,	1]
}




dataset = pd.DataFrame(data)
# we can predict the result that is ... can a person buy a car or which person can buy a are and its age so here no use to take uid and sex column.
# So we will take only age,salary and purchased column

X = dataset.iloc[:,0:2].values
y = dataset.iloc[:,3].values


# Split the data into train and test set
from sklearn.cross_validation import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

# Fitting Logistic Regression to Training set
from sklearn.svm import SVC
regresssor = SVC(kernel = 'linear',random_state = 2)
# This is Linear Classifier 
# So it is look like logistic regression
regresssor.fit(X_train, y_train)

#Predicting the Test set results
y_pred = regresssor.predict(X_test)

# Here we will see that is our logistic regresssor make correct prediction or not
# Confusion matrix will help us to get this

# Create Confusion Matrix
from sklearn.metrics import confusion_matrix
#confusion_matrix is a function because it does not started with capital letter

cm = confusion_matrix(y_test, y_pred)
# first parameter is y_true means true value 
# here true value is y_test

# Second parameter is predicted values
print(cm)
# this will give this result means
# array([[65,  2],
#       [ 8, 24]])
# 65 and 24 is correct prediction
# and 8 and 3 is incorrect prediction

# total 89 correct and 11 incorrect prediction out of 
# 100


#  But most accurate way to evalute a model is graphical model

from matplotlib.colors import ListedColormap
def LogisticPlot(X_set, y_set):
    X1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() + 1, step = 0.01),
    np.arange(start = X_set[:,1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
    # print(X_set[:,0].min() - 1)
    # print(X_set[:,0].max() + 1)
    # print(X_set[:,1].min() - 1)
    # print(X_set[:,1].max() + 1)
    # print(X1, X2)
    plt.contourf(X1, X2, regresssor.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),alpha = 0.75, cmap =     
    ListedColormap(('red', 'green')))

    plt.xlim(X1.min(), X1.max())
    plt.ylim(X2.min(), X2.max())
    for i,j in enumerate(np.unique(y_set)):
         plt.scatter(X_set[y_set == j, 0], X_set[y_set == j,1], c = ListedColormap(('yellow' 
         ,'black'))(i), label = j)
    
    plt.title('SVM Regression(Training set)')
    plt.xlabel('Age')
    plt.ylabel('Estimated Salary')
    plt.legend()
    plt.show()
    return X1 , X2
    
X1 ,X2 = LogisticPlot(X_train, y_train)
X1, X2 = LogisticPlot(X_test, y_test)

print( X1,  X2 )
